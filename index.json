[{"content":"EazyPredict - Running and comparing multiple ML models at once # Welcome to the world of \u0026lsquo;EazyPredict\u0026rsquo;, a Python module that aims to make trying out multiple prediction algorithms as simple and efficient as possible. The module was heavily influenced by the \u0026lsquo;LazyPredict\u0026rsquo; module. I developed this module to address a few shortcomings I identified in LazyPredict.\nWhy EazyPredict? # Some of its key features are as follows -\nThe \u0026lsquo;EazyPredict\u0026rsquo; module utilizes a limited number of prediction algorithms (10) in order to minimize memory usage and prevent potential issues on platforms such as Kaggle.\nUsers have the option to input a custom list of prediction algorithms (as demonstrated in the example provided) in order to perform personalized comparisons with estimators of their choosing.\nThe models can be saved to an output folder at the user\u0026rsquo;s discretion and are returned as a dictionary, allowing for easy addition of custom hyperparameters.\nThe top N models can be selected to create an ensemble using a voting classifier.\nUsing it for classification # Let\u0026rsquo;s try it on this introductory problem on kaggle.\nAs written on kaggle -\n\u0026ldquo;This is the legendary Titanic ML competition – the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works. The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\u0026rdquo;\nFirst, we need to load the dataset:\ndf = pd.read_csv(\u0026#34;data/train.csv\u0026#34;) df.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S So before using eazypredict, we need to pre-process the dataset. This includes the following steps -\nRemoving null values Encoding categorical data Scaling the dataset Splitting the training and testing data # Removes null values df[\u0026#34;Age\u0026#34;].fillna(method=\u0026#34;bfill\u0026#34;, inplace=True) df[\u0026#34;Cabin\u0026#34;].fillna(\u0026#34;No Room\u0026#34;, inplace=True) df[\u0026#34;Embarked\u0026#34;].fillna(\u0026#34;S\u0026#34;, inplace=True) # Encodes categorical data ord_enc = OrdinalEncoder() df[\u0026#34;Sex_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Sex\u0026#34;]]) df[\u0026#34;Cabin_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Cabin\u0026#34;]]) df[\u0026#34;Embarked_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Embarked\u0026#34;]]) # Selects features for X and labels for y X_feat = [ \u0026#34;Pclass\u0026#34;, \u0026#34;Age\u0026#34;, \u0026#34;SibSp\u0026#34;, \u0026#34;Parch\u0026#34;, \u0026#34;Fare\u0026#34;, \u0026#34;Sex_code\u0026#34;, \u0026#34;Cabin_code\u0026#34;, \u0026#34;Embarked_code\u0026#34;, ] y_feat = [\u0026#34;Survived\u0026#34;] X = df[X_feat] y = df[y_feat] # Scaling the features scaler = RobustScaler() X_norm = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) # Splitting into train, set X_train, X_test, y_train, y_test = train_test_split( X_norm, y, test_size=0.33, random_state=42 ) X_norm.head() Pclass Age SibSp Parch Fare Sex_code Cabin_code Embarked_code 0 0.0 -0.388889 1.0 0.0 -0.312011 0.0 0.0 0.0 1 -2.0 0.500000 1.0 0.0 2.461242 -1.0 -65.0 -2.0 2 0.0 -0.166667 0.0 0.0 -0.282777 -1.0 0.0 0.0 3 -2.0 0.333333 1.0 0.0 1.673732 -1.0 -91.0 0.0 4 0.0 0.333333 0.0 0.0 -0.277363 0.0 0.0 0.0 Now we can use eazypredict module to quicly get the predictions of the top classification algorithms.\nclf = EazyClassifier() model_list, prediction_list, model_results = clf.fit(X_train, X_test, y_train, y_test) model_results 100%|██████████| 10/10 [00:00\u0026lt;00:00, 10.09it/s] Accuracy f1 score ROC AUC score GaussianNB 0.803390 0.803637 0.797619 MLPClassifier 0.803390 0.800228 0.784524 RandomForestClassifier 0.800000 0.798956 0.788214 LGBMClassifier 0.800000 0.798244 0.785595 RidgeClassifier 0.796610 0.794629 0.781429 XGBClassifier 0.779661 0.779203 0.769762 DecisionTreeClassifier 0.779661 0.778869 0.768452 KNeighborsClassifier 0.769492 0.766785 0.752024 SVC 0.688136 0.662186 0.640238 SGDClassifier 0.681356 0.669167 0.647619 After this you have the ability to select any model and perform hyperparameter tuning on it.\ngaussian_clf = model_list[\u0026#34;GaussianNB\u0026#34;] from sklearn.model_selection import GridSearchCV params_NB = {\u0026#34;var_smoothing\u0026#34;: np.logspace(0, -9, num=100)} gs_NB = GridSearchCV( estimator=gaussian_clf, param_grid=params_NB, verbose=1, scoring=\u0026#34;accuracy\u0026#34; ) gs_NB.fit(X_train, y_train.values.ravel()) gs_NB.best_params_ Fitting 5 folds for each of 100 candidates, totalling 500 fits {'var_smoothing': 8.111308307896873e-06} Using it for regression # It can be used for regression in pretty much the same way as above. You just need to import the EazyRegressor estimator.\nMore details can be found here .\nCreating an ensemble model # This is the most effective feature of this library as an ensemble model can create a really good model with minimal effort in hyper parameter tuning.\nAll you need to do is to pass the results and the model names from the previous \u0026ldquo;fit\u0026rdquo; step to the next one.\nclf = EazyClassifier() model_list, prediction_list, model_results = clf.fit(X_train, X_test, y_train, y_test) ensemble_reg, ensemble_results = clf.fitVotingEnsemble(model_list, model_results) ensemble_results 100%|██████████| 10/10 [00:01\u0026lt;00:00, 6.68it/s]\nModels Accuracy F1 score ROC AUC score 0 GaussianNB LGBMClassifier RidgeClassifier MLPC... 0.816949 0.758929 0.799881 Conclusion # In conclusion, \u0026lsquo;EazyPredict\u0026rsquo; is an efficient and user-friendly Python module that makes trying out multiple prediction algorithms a breeze. Its memory-efficient design and customizable options make it a valuable tool for any data scientist or machine learning enthusiast. I hope you enjoy using \u0026lsquo;EazyPredict\u0026rsquo; as much as I enjoyed creating it.\nCheck out the entire project on Github or PyPI .\n","date":"2 February 2023","permalink":"/posts/2023-02-03-eazypredict-module/","section":"Posts","summary":"EazyPredict - Running and comparing multiple ML models at once # Welcome to the world of \u0026lsquo;EazyPredict\u0026rsquo;, a Python module that aims to make trying out multiple prediction algorithms as simple and efficient as possible.","title":"EazyPredict ML module"},{"content":"","date":"2 February 2023","permalink":"/tags/from-scratch/","section":"Tags","summary":"","title":"from-scratch"},{"content":"","date":"2 February 2023","permalink":"/tags/k-nearest-neighbours/","section":"Tags","summary":"","title":"k-nearest-neighbours"},{"content":"","date":"2 February 2023","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine-learning"},{"content":"","date":"2 February 2023","permalink":"/","section":"My Portfolio","summary":"","title":"My Portfolio"},{"content":"","date":"2 February 2023","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"2 February 2023","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":"2 February 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" In this blog we go through the kNN algorithm logic, implement it from scratch in python and apply it on the MNIST digit dataset\nkNN algorithm explained intuitively! # Prerequisites # machine learning, supervised vs unsupervised ML, classification vs regression\nIntroduction # The K Nearest Neighbour classification is a simple and efficient machine learning model that can used for classification. While there are more complex classification models, if establishing a model with some training dataset is proving troublesome, it is likely that a kNN algorithm will provide the best solution. kNN is considered to be one of the top 10 data mining algorithms for this reason.\nkNN is not a model based classifier, instead it is called a lazy learner, as it performs classification tasks without building its model. In other words, the model does not have a “training” phase like other common machine learning models. The reasons for this will become apparent after I explain the model with an example.\nThis algorithm works on the concept of “similarity” between data points. This can easily be visualized by using a 2 dimensional dataset.\nExplanation with dummy data # Let’s say we wish to create a machine learning model that knows how to classify images of cats and dogs. For any ML model, we must have some data that highlights the different characteristics of both animals. Let us create this dataset ourselves, that would be pretty fun! The two characteristics that we choose could be as follows -\nThe sharpness of its claws The length of its ears Using these two features, we can end up with a model that has a reasonable accuracy. First, lets build a dataset for the problem. I’ll be doing this by measuring 4 legged mammals in my area ;). brb!\nDisclaimer - No animals were harmed in the creation of this blog\nLet\u0026rsquo;s say the data I have is as follows -\nAnimal Sharpness of its claws (cm) Length of its ears (cm) Cat 4 1 Cat 5 2 Cat 4 3 Dog 1 3 Dog 2 4 Dog 2 6 This data is classified into two classes, cat and dog. The data consists of two columns, the first one is called the label column. This identifies which class a data point is supposed to be in, and the second column is our features column.\nPlotting them results in a graph as shown below. Notice how similar animals are grouped “together” in the plot? This property is what kNN uses to perform classification.\nNow that we have our training data, let’s introduce a new unknown animal with the features (2.5, 4). Our model needs to able to determine if the unknown animal is a cat or a dog i.e. which class it belongs to. (Spoiler alert: it’s a cat! But our model doesn’t know that :p)\nThe nearest neighbour algorithm finds the data point that is “closest” to our unknown point. The way we measure this “closeness” can be through common methods, like euclidean or cosine distance. Here, I will be using the euclidean distance equation for the demonstration.\n$$ d(p,q) = sqrt{\\sum_{i=1}^n (q_i - p_i)^2} $$\nIf we calculate the euclidean distance from all other points and find the shortest distance, we can infer that the closest point to our unknown point is the point (2,4). This point refers to a cat, and so the nearest neighbour algorithm concludes that the unknown point is a cat. However, this method is extremely prone to overfitting, or in other words, if there are a few outliers in our data, the model’s prediction will suffer.\nWe overcome this issue by using the K Nearest Neighbour algorithm, which also happens to be the topic of this article :). What changes here is, instead of finding ONE shortest point, we find K points which are sorted in the order of their distance to our unknown point, in ascending order. From these K points, we look at which class value occurs the most, and then that will be the predicted class of the unknown point.\nLets take K=3 for our example. This will include the points given below. Since there are 2 points from the class “Cat”, and one point from the class “Dog”, our kNN model will predict (correctly) that our unknown animal is a cat.\nThat is the working of the kNN model on a very simple dataset. Now in real world machine learning problems you do not work on such simple datasets, but this example helps build our intuition of what the model does.1\nCoding kNN model from scratch # Now let’s create this model from scratch in python. Since it is a lazy learning algortihm, we do not need a training phase. We just need the training data and an un labelled instance for prediction. So the algorithm needs features, labels, and the data instance to be predicted as an input. We\u0026rsquo;ll choose an arbitrary value for K for this example.2 We also need a helper function to calculate the euclidean distance between two vectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # returns the euclidean distance between two vectors def euclidean_distance(vec1, vec2): distance=0 for i in range(len(vec1)): distance+=(vec1[i]-vec2[i])**2 return math.sqrt(distance) def knn_function(X_train, y_train, test_vector, k): distance=[] # loops through the training dataset, stores the training vector and corresponding label in a variable for idx in range(len(X_train)): img=X_train[idx] label=y_train[idx] # appends a tuple containing euclidean distance and label of the corresponding training vector distance.append((euclidean_distance(img, test_vector), label)) # sorts the tuples in the list in descending order, according to the euclidean distance sorted_dist=sorted(distance, key=lambda dist:dist[0]) # takes the first k values in the list i.e the k nearest neighbours neighbours=np.array(sorted_dist[:k]) return neighbours That\u0026rsquo;s the whole code! It really is a pretty simple learning algorithm. Now all we need a wrapper function to accept the input and return the prediction to the user.\n1 2 3 4 5 # this returns the predicted label i.e. the label that occurs the most def predict_class(test_vector, k=5): neighbours=knn_function(X_train, y_train, test_vector, k) labels, counts = np.unique(neighbours[:,1], return_counts=True) return labels[counts.argmax()] Please not that our implementation is not very optimized and really should be only used to learn how the model works. 3\nApplying the model on a standard dataset # As written on kaggle -\n\u0026ldquo;MNIST (\u0026ldquo;Modified National Institute of Standards and Technology\u0026rdquo;) is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\u0026rdquo;\nSo it\u0026rsquo;s a pretty standard dataset! Let\u0026rsquo;s use this to evaluate our model\u0026rsquo;s performance! First let\u0026rsquo;s get the dataset from kaggle and read it using pandas!\nThe shape of this data is (60000, 785). This means that each row is a vector containing a digit representation. That comes out to a total of 60000 images in the training dataset. The digit is represented as a 1x784 vector. So then why is the shape 785? This is because the last column is a label that stores what digit is represented by that particular row.\nThen we use numpy to store the data into train, test datasets.\n1 2 3 4 5 6 7 8 9 10 #split into features and labels X_train = np.array(train_data[train_data.columns.drop(\u0026#39;label\u0026#39;)]) y_train = np.array(train_data[\u0026#39;label\u0026#39;]) X_test = np.array(test_data[test_data.columns.drop(\u0026#39;label\u0026#39;)]) y_test = np.array(test_data[\u0026#39;label\u0026#39;]) # shuffle the data randomly rand_idx = np.random.permutation(X_train.shape[0]) X_train = X_train[rand_idx] y_train = y_train[rand_idx] Now let\u0026rsquo;s see what an image from this dataset looks like, and print the label.\n1 2 3 4 5 6 7 8 img=X_test[31] img = img.reshape(28, 28) plt.imshow(img, cmap=matplotlib.cm.binary) plt.axis(\u0026#39;off\u0026#39;) plt.show() print(y_test[31]) img.shape OUTPUT: 1.0 That looks pretty good! This means that the 31st vector stored in out test dataset corresponds to this image of the digit \u0026lsquo;1\u0026rsquo;. So now we have confirmed that the data is indeed stored and labelled properly. It\u0026rsquo;s time to unleash our kNN model on this dataset! Let us try to predict the result we get for the same test input. If everything goes well, we should get \u0026lsquo;1\u0026rsquo; as the predicted output.\nAnd we do! Our predict function predicted correctly that the image shown was that of a 1. Here also, the k value that we passed to the function was completely arbitrary. Now let\u0026rsquo;s properly evaluate the model on the entire test set.\nThe accuracy comes out to about 90%. Now I should say that this accuracy can be improved by selecting a better value of k, but it\u0026rsquo;s not bad for a model without any hyperparameter tuning.\nFootnotes # You could extend the same principles to higher dimensions. Here we have 2 features, so we only have to visualize 2 dimensions. But for most problems, it\u0026rsquo;s common for there to be a lot of dimensions, millions even. It’s difficult to imagine problems in a higher dimension, so working on them in lower dimensions while learning is a good idea! Quick detour on this topic here .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt should be noted that there does exist algorithms for choosing an ideal value of K based on the training dataset. You can learn more here .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOptimizations can be made to the algorithm such that some of the computation can be skipped, but this discussion is out of the scope of this article. Feel free to read up on it here \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"30 January 2022","permalink":"/posts/2022-01-30-knn-algorithm-demystified/","section":"Posts","summary":"In this blog we go through the kNN algorithm logic, implement it from scratch in python and apply it on the MNIST digit dataset\nkNN algorithm explained intuitively! # Prerequisites # machine learning, supervised vs unsupervised ML, classification vs regression","title":"kNN algorithm explained intuitively!"},{"content":"Chrome Extension To Monitor Mental Health # Introduction # This project was a really interesting one that I was fortunate enough to be a part of! We made as an entry project to compete in a hackathon organised by HackOn. We did not win the hackathon, but we did create a unique and interesting project. (imo)\nOur project aims to monitor a user\u0026rsquo;s search patterns and use this information to inform their close/loved ones about potential mental health problems the user might be facing. A recent study showed that most people who are mentally depressed in some form have used the internet in a manner that has worsened their illness. So our goal with this approach is to help in the monitoring of ‘at-risk’ individuals and prevent them from becoming their own worst enemy.\nWe aim to achieve this goal through a chrome extension that tracks the search phrases entered by the user and sends them to a deep learning model, that determines whether the user shows signs of depression or not!\nThought Process Behind the Project # The motivation behind this project is highly personal. We all have a friend who had to go through some dark times in some periods of their lives. Some were able to overcome this, while some were not so fortunate. One thing that we all can agree on is that there are not many support systems that help or support people in such cases. Our extension is aimed to help not only the fighters but also their supporters who want to help more. We know someone who went through such a phase, and something that was missing was a medium to alert close ones when to intervene, as you can never expect someone to know everything you are going through.\nMaking the machine learning model # I was in charge of making the machine learning model that would be used by us for the web extension. I used common NLP methods such as preprocessing, tokenizing, encoding. The pre-processing was done by making a function of its own.\nDataset # We used two different datasets for this model. The problem is a classification question that has to decide whether a person is depressed (true state) or not depressed (false state). I used 2 datasets to train the true and false state.\nThe first dataset is from kaggle. It is a collection of posts from \u0026ldquo;SuicideWatch\u0026rdquo; and \u0026ldquo;depression\u0026rdquo; subreddits of the Reddit platform. The posts are collected using Pushshift API. All posts that were made to \u0026ldquo;SuicideWatch\u0026rdquo; from Dec 16, 2008 (creation) till Jan 2, 2021, were collected while \u0026ldquo;depression\u0026rdquo; posts were collected from Jan 1, 2009, to Jan 2, 2021. We took the posts that were written by people suffering from depression as the true state.\nThe second dataset is a sentiment analysis dataset. It contains 1,600,000 tweets extracted using the twitter API. The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment. We took the dataset that includes neutral and positive flags.\nI did it this way because the \u0026rsquo;negative\u0026rsquo; statements from the twitter sentiment analysis dataset might be some tweet regarding a hate crime, or an angry tweet. It does not necessarily have to be the words of a person undergoing depression. So I overcame this with the Reddit dataset. Going through the subreddit mentioned, it was clear that most of the posts were regarding depression/existential crisis. Any post swaying from these topics would be quickly taken down by the moderators. So, this was most accurate data we would get that could be used to emulate the psyche of a depressed person.\nPre-proccessing- # TEXT_CLEANING_RE = \u0026#34;@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\u0026#34; def preprocess(text, stem=False): # Remove link,user and special characters text = re.sub(TEXT_CLEANING_RE, \u0026#39; \u0026#39;, str(text).lower()).strip() tokens = [] for token in text.split(): if token not in stop_words: if stem: tokens.append(stemmer.stem(token)) else: tokens.append(token) return \u0026#34; \u0026#34;.join(tokens) I used packages from word2vec and keras text pre-processing for building the vocabulary and tokenizing the data respectively.\nBuilding vocabulary - # W2V_SIZE = 300 W2V_WINDOW = 7 W2V_EPOCH = 32 W2V_MIN_COUNT = 10 KERAS_MODEL = \u0026#34;model.h5\u0026#34; WORD2VEC_MODEL = \u0026#34;model.w2v\u0026#34; TOKENIZER_MODEL = \u0026#34;tokenizer.pkl\u0026#34; ENCODER_MODEL = \u0026#34;encoder.pkl\u0026#34; w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8) w2v_model.build_vocab(documents) w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH) Tokenizing - # tokenizer = Tokenizer() tokenizer.fit_on_texts(df_train.text) vocab_size = len(tokenizer.word_index) + 1 print(\u0026#34;Total words\u0026#34;, vocab_size) Then I used LabelEncoder() from sklearn to encode the tokenized text data.\nEncoding - # encoder = LabelEncoder() encoder.fit(df_train.target.tolist()) y_train = encoder.transform(df_train.target.tolist()) y_test = encoder.transform(df_test.target.tolist()) y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) Making the model # We defined the neural network as follow and trained it for about 10 epochs. This resulted in an accuracy of 96%.\nmodel = Sequential() model.add(embedding_layer) model.add(Dropout(0.5)) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.summary() model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#34;adam\u0026#34;, metrics=[\u0026#39;accuracy\u0026#39;]) After the training is done, we can predict if a phrase shows signs of depression or not, in the following way -\npredict(\u0026#34;painless way to die\u0026#34;, model1) {\u0026#39;elapsed_time\u0026#39;: 0.1011207103729248, \u0026#39;label\u0026#39;: True, \u0026#39;score\u0026#39;: 0.8831092119216919} Making the chrome extension # I was not involved very heavily in this part, but what we did was to design a front end that takes in a users email ID, name and multiple friend\u0026rsquo;s email IDs. The extension is built using javascript and closely interacts with Chrome extensions API. This makes it possible to run on any chromium based browser such as opera, edge, etc. The extension gets the search query and sends it as a JSON structure to our API endpoint /predict. This endpoint is where our DL model is hosted. The DL model takes the string as an input and returns a score (from 0-1 ). The higher the score is, the more likely the person is to be depressed. This extension runs in the background and updates a score variable based on the averages of multiple search results. If the score variable crosses a particular threshold, an email will be sent to the close ones of the user.\nThat was the method we used to implement this project :). Link to the GitHub repo can be found here. ","date":"31 May 2021","permalink":"/posts/2021-05-31-notblue-chrome-extension/","section":"Posts","summary":"Chrome Extension To Monitor Mental Health # Introduction # This project was a really interesting one that I was fortunate enough to be a part of! We made as an entry project to compete in a hackathon organised by HackOn.","title":"Chrome Extension To Monitor Mental Health"},{"content":"","date":"31 May 2021","permalink":"/tags/machine-learning-natural-language-processing-deep-learning-chrome-extension/","section":"Tags","summary":"","title":"machine-learning, natural-language-processing, deep-learning, chrome-extension"},{"content":"AMFOSS TASKS # \u0026ldquo;Success is normally found in a pile of mistakes.\u0026rdquo; - Tim Fargo\nIf there was ever a point in my life where I felt this quote, it is after I started doing the tasks required for joining the amFOSS club at Amrita Vishwa Vidyapeetham.\nAmFOSS is an open source club consisting of a few motivated students promoting and contributing to free and open source software. This helps students to learn out of academics and get introduced to the outside world. So, quite naturally I was interested in joining the club. I came to know about this club through quora. There were a lot of posts on the club(which I found out later was a result of foss members spamming the website).\ntask-1,2 # My first direct introduction to amFOSS, however was through the induction ceremony organized by the college. I registered for the club immediately, and soon got the tasks. At first, I was feeling quite good about the tasks. Task 1 was a breeze, all I had to do was run an automated script on GitHub. Task 2 was a bunch of typical programming challenges. I had learnt python during my vacations as I had way too much free time on my hand. That came in handy and I did the challenges using python. TASK 1 TASK 2 task 3 # After these, I started working on task 3. This task was very close to making me successfully pull my hair out. I had to create a program for scraping google search using ruby and nokogiri. I read through most of the documentation of ruby and nokogiri. I also scraped a smaller website as a part of a blog post tutorial. So I was feeling quite confident in cracking this task; which was until I actually started doing the task. I had underestimated the complexity of a google search page, and no matter what I tried, I could not parse any sort of useful result from it. I immediately pinned this on my lack of knowledge of HTML and CSS. To rectify this issue, I started learning HTML and CSS from an online source. It took me about two days to learn the syntaxes of HTML and CSS.(Even though it was explicitly mentioned in the pdf given to us to not get sidetracked into learning a whole language, I couldn\u0026rsquo;t stop until I felt that I had knowledge of the basics). This did give me clearer understanding of the problem and made the use of nokogiri much easier, but at the end of the day, I had no results to display. It almost seemed to me that the google search page was built in a way to prevent scraping. That is as far a progress I got in that task. TASK 3 task-4 # Dejected at not being able to solve the task, I moved on to the next task, \u0026ldquo;Advanced XOR\u0026rdquo;. I was completely new to encryption of all sorts. So, I read about encryption and learnt what the terms key, check hash, ciphertext stood for. Then I proceeded to read the encryption script. I tried doing it for a bit of time, but then I simply could not understand it.\ntask-5 # This brings us to the next task. I started this task with apprehension because graphQL was a relatively new language. I started out with reading about API\u0026rsquo;s and the different ways they are used to query information. Then I read about rEST API, I understood that graphQL was an improvement to rEST since several instances of data could be returned in a single query compared to rEST which requires multiple queries to get the same results. After aquiring this knowledge I started working on my website. This proved to be relatively simple and I finished it in about a day. I also read about graphQL on multiple websites and since there wasn\u0026rsquo;t much information on it, I relied heavily on it\u0026rsquo;s official documentation. Graphiql explorer was a fun way of trying graphQl and I started experimentation. Since the syntax was easy, I got the query working there. This is where I ran into my blockers. I had two main blockers in this task and they haunt me to this day. They were - i) Authenticating the query from javascript. ii) Implementing graphQL in javascript. This took up about 1/3rd of my total time and was a huge pain in the ass. I looked at a variety of libraries and clients to resolve the issue. They include graphql.js, nodejs, apollo client for graphql. I even went as far trying to execute it through a python script using django after reading through the method in which they implemented it on GitLit repositary on amFOSS directly. Needless to say, I learnt how not to approach an issue through this task. I wasted a lot of time on this that could be used in other tasks. TASK 5 task 6,7 # I made negligible progress in tasks 6,7. All I did was study the syntax of rust and installed it on my laptop.\ntask 8 # Captcha breaking was a very simple task and it was a welcome addition after task 5. All I did was install a couple of packages from google.(Tesseract OCR). After that it was fairly straightforward to get the text from images using the OCR. TASK 8 task 9 # Creating a website using jekyll themes was also pretty straightforward. Here, the knowledge I got by learning HTML, CSS came into handy and editing the website was a breeze. I found out this super cool minimalistic theme from jekyll themes, forked it and got a website without much fuss. TASK 9 ","date":"20 August 2019","permalink":"/posts/2019-08-20-amfoss-tasks/","section":"Posts","summary":"AMFOSS TASKS # \u0026ldquo;Success is normally found in a pile of mistakes.\u0026rdquo; - Tim Fargo\nIf there was ever a point in my life where I felt this quote, it is after I started doing the tasks required for joining the amFOSS club at Amrita Vishwa Vidyapeetham.","title":"Applying to AMFOSS open source club"},{"content":"","date":"20 August 2019","permalink":"/tags/beginnner/","section":"Tags","summary":"","title":"beginnner"},{"content":"","date":"20 August 2019","permalink":"/tags/programming/","section":"Tags","summary":"","title":"programming"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]