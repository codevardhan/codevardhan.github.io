[{"content":"","date":"20 May 2023","permalink":"/about/","section":"My Portfolio","summary":"","title":"About Me"},{"content":"","date":"20 May 2023","permalink":"/","section":"My Portfolio","summary":"","title":"My Portfolio"},{"content":"Harshavardhan M\nB.Tech in Computer Science, specialized in Artificial Intelligence\nEducation # Degree/Certificate Institute/Board CGPA/Percentage Year B.Tech. Amrita Vishwa Vidyapeetham, Kollam 8.44 2019-2023 Senior Secondary CBSE Board 92.0% 2019 Secondary CBSE Board 100% 2017 Experience # Agrisoft Diary and Agro Producer Company Ltd. (Jan 2022 - April 2022, Kollam)\nFull Stack Developer Intern Designed and implemented a novel ERP software pipeline. Worked on the development of two Flutter applications with Firebase as the backend. Developed a REST API using Express and created a custom user interface to access the backend. Designed and integrated a payment solution tailored to client needs. Led weekly client meetings to facilitate open communication and ensure seamless project development. AmritaCREATE Labs (June 2022 - April 2023, Amritapuri)\nDeep Learning Research Intern An edu-tech research initiative that are funded principally by research grants. Working on Sign Language Accessibility for e-Governance Services at Amrita CREATE. We use AI/ML for sign language recognition of questions, that is asked to the chatbot in FAQs across 25 UMANG services. Developed a Deep Learning pipeline and successfully implemented a custom BERT model tailored to address this specific problem. Created a pioneering solution by integrating heatmaps into the pretraining process of a transformer-based model. Projects # Not Blue Chrome Extension (Deep Learning/NLP) (May 2021)\nDeveloped a chrome extension that monitors the search activity of a user to identify potential signs of depression. Utilized a deep learning LSTM model to calculate a sentiment score for each search phrase. Implemented a threshold mechanism to detect concerning levels of sentiment, triggering an email notification to the user\u0026rsquo;s friends and family. Tools \u0026amp; technologies used: Tensorflow, Flask, Javascript Packet Sniffer (Computer Networking) (October 2021)\nDeveloped a simple packet sniffing tool using Python. Implemented the tool to capture data on a raw socket and provided an interactive dashboard to display network usage and other relevant information. Worked on the Flask back-end and parts of the front end. Tools \u0026amp; technologies used: Python, Flask, HTML, Bootstrap, JavaScript, and jQuery. Space Invaders using Deep Q-Learning (Reinforcement Learning) (May 2022)\nA bot that plays the classic atari game, Space Invaders. Space Invaders is a classic japanese shooting video game that was released for Atari 6000. This bot was trained using a convolutional neural network as a feature extractor. It was then trained using the dueling neural network strategy. Tools \u0026amp; technologies used: Python, Keras, Arcade Learning Environments Technical Skills And Spoken Languages # Programming: Python, Java, MATLAB, R, HTML/CSS, Javascript, Dart, Bash, Kotlin Tools \u0026amp; OS: Visual Studio, Jupyter Notebook, Google Collab, Git, Flutter, Android Studio, Gazebo Libraries/Frameworks: Pandas, Numpy, PyTorch, Tensorflow, nodeJS Languages: English, Hindi, Malayalam Awards and Extra Curricular Activities # AMFOSS (2019)\nOpen source coding club Contributed to open source repositories in the fields of Android app development and web development. Actively participated in diverse club activities, including attending tech talks and hackathons. Ayudh Amritapuri (2019-2022)\nInternational Non-Governmental Organization Volunteer for Amala Bharatam Campaign, AYUDH India, participated in clean-up drives and organized awareness drives in 7 venues with a team of 200+ in August 2019. Conducted multiple webinars on \u0026ldquo;Open source software and why you should get started\u0026rdquo; for underprivileged high school graduates, helping them embark on their software journey. AI at Amrita (2020-2022)\nCoding club for AI/ML developers Worked on multiple machine learning pipelines and actively participated in numerous competitions and hackathons. Conducted informative seminars on machine learning algorithms to support the learning and development of newer club members. Cubers at Amrita (2020-2023)\nSpeed cubing club for Rubik’s cube enthusiasts. Participated in and helped organize multiple international speed cubing events recognized by the World Cube Association. Developed strong collaboration skills within the team, facilitating the on-boarding of new club members. ","date":"20 May 2023","permalink":"/resume/","section":"My Portfolio","summary":"Harshavardhan M\nB.Tech in Computer Science, specialized in Artificial Intelligence\nEducation # Degree/Certificate Institute/Board CGPA/Percentage Year B.Tech. Amrita Vishwa Vidyapeetham, Kollam 8.44 2019-2023 Senior Secondary CBSE Board 92.0% 2019 Secondary CBSE Board 100% 2017 Experience # Agrisoft Diary and Agro Producer Company Ltd.","title":"My Resume"},{"content":"","date":"20 May 2023","permalink":"/tags/resume/","section":"Tags","summary":"","title":"resume"},{"content":"","date":"20 May 2023","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"EazyPredict - Running and comparing multiple ML models at once # Welcome to the world of \u0026lsquo;EazyPredict\u0026rsquo;, a Python module that aims to make trying out multiple prediction algorithms as simple and efficient as possible. The module was heavily influenced by the \u0026lsquo;LazyPredict\u0026rsquo; module. I developed this module to address a few shortcomings I identified in LazyPredict.\nWhy EazyPredict? # Some of its key features are as follows -\nThe \u0026lsquo;EazyPredict\u0026rsquo; module utilizes a limited number of prediction algorithms (10) in order to minimize memory usage and prevent potential issues on platforms such as Kaggle.\nUsers have the option to input a custom list of prediction algorithms (as demonstrated in the example provided) in order to perform personalized comparisons with estimators of their choosing.\nThe models can be saved to an output folder at the user\u0026rsquo;s discretion and are returned as a dictionary, allowing for easy addition of custom hyperparameters.\nThe top N models can be selected to create an ensemble using a voting classifier.\nUsing it for classification # Let\u0026rsquo;s try it on this introductory problem on kaggle.\nAs written on kaggle -\n\u0026ldquo;This is the legendary Titanic ML competition – the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works. The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\u0026rdquo;\nFirst, we need to load the dataset:\ndf = pd.read_csv(\u0026#34;data/train.csv\u0026#34;) df.head() PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S So before using eazypredict, we need to pre-process the dataset. This includes the following steps -\nRemoving null values Encoding categorical data Scaling the dataset Splitting the training and testing data # Removes null values df[\u0026#34;Age\u0026#34;].fillna(method=\u0026#34;bfill\u0026#34;, inplace=True) df[\u0026#34;Cabin\u0026#34;].fillna(\u0026#34;No Room\u0026#34;, inplace=True) df[\u0026#34;Embarked\u0026#34;].fillna(\u0026#34;S\u0026#34;, inplace=True) # Encodes categorical data ord_enc = OrdinalEncoder() df[\u0026#34;Sex_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Sex\u0026#34;]]) df[\u0026#34;Cabin_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Cabin\u0026#34;]]) df[\u0026#34;Embarked_code\u0026#34;] = ord_enc.fit_transform(df[[\u0026#34;Embarked\u0026#34;]]) # Selects features for X and labels for y X_feat = [ \u0026#34;Pclass\u0026#34;, \u0026#34;Age\u0026#34;, \u0026#34;SibSp\u0026#34;, \u0026#34;Parch\u0026#34;, \u0026#34;Fare\u0026#34;, \u0026#34;Sex_code\u0026#34;, \u0026#34;Cabin_code\u0026#34;, \u0026#34;Embarked_code\u0026#34;, ] y_feat = [\u0026#34;Survived\u0026#34;] X = df[X_feat] y = df[y_feat] # Scaling the features scaler = RobustScaler() X_norm = pd.DataFrame(scaler.fit_transform(X), columns=X.columns) # Splitting into train, set X_train, X_test, y_train, y_test = train_test_split( X_norm, y, test_size=0.33, random_state=42 ) X_norm.head() Pclass Age SibSp Parch Fare Sex_code Cabin_code Embarked_code 0 0.0 -0.388889 1.0 0.0 -0.312011 0.0 0.0 0.0 1 -2.0 0.500000 1.0 0.0 2.461242 -1.0 -65.0 -2.0 2 0.0 -0.166667 0.0 0.0 -0.282777 -1.0 0.0 0.0 3 -2.0 0.333333 1.0 0.0 1.673732 -1.0 -91.0 0.0 4 0.0 0.333333 0.0 0.0 -0.277363 0.0 0.0 0.0 Now we can use eazypredict module to quicly get the predictions of the top classification algorithms.\nclf = EazyClassifier() model_list, prediction_list, model_results = clf.fit(X_train, X_test, y_train, y_test) model_results 100%|██████████| 10/10 [00:00\u0026lt;00:00, 10.09it/s] Accuracy f1 score ROC AUC score GaussianNB 0.803390 0.803637 0.797619 MLPClassifier 0.803390 0.800228 0.784524 RandomForestClassifier 0.800000 0.798956 0.788214 LGBMClassifier 0.800000 0.798244 0.785595 RidgeClassifier 0.796610 0.794629 0.781429 XGBClassifier 0.779661 0.779203 0.769762 DecisionTreeClassifier 0.779661 0.778869 0.768452 KNeighborsClassifier 0.769492 0.766785 0.752024 SVC 0.688136 0.662186 0.640238 SGDClassifier 0.681356 0.669167 0.647619 After this you have the ability to select any model and perform hyperparameter tuning on it.\ngaussian_clf = model_list[\u0026#34;GaussianNB\u0026#34;] from sklearn.model_selection import GridSearchCV params_NB = {\u0026#34;var_smoothing\u0026#34;: np.logspace(0, -9, num=100)} gs_NB = GridSearchCV( estimator=gaussian_clf, param_grid=params_NB, verbose=1, scoring=\u0026#34;accuracy\u0026#34; ) gs_NB.fit(X_train, y_train.values.ravel()) gs_NB.best_params_ Fitting 5 folds for each of 100 candidates, totalling 500 fits {'var_smoothing': 8.111308307896873e-06} Using it for regression # It can be used for regression in pretty much the same way as above. You just need to import the EazyRegressor estimator.\nMore details can be found here .\nCreating an ensemble model # This is the most effective feature of this library as an ensemble model can create a really good model with minimal effort in hyper parameter tuning.\nAll you need to do is to pass the results and the model names from the previous \u0026ldquo;fit\u0026rdquo; step to the next one.\nclf = EazyClassifier() model_list, prediction_list, model_results = clf.fit(X_train, X_test, y_train, y_test) ensemble_reg, ensemble_results = clf.fitVotingEnsemble(model_list, model_results) ensemble_results 100%|██████████| 10/10 [00:01\u0026lt;00:00, 6.68it/s]\nModels Accuracy F1 score ROC AUC score 0 GaussianNB LGBMClassifier RidgeClassifier MLPC... 0.816949 0.758929 0.799881 Conclusion # In conclusion, \u0026lsquo;EazyPredict\u0026rsquo; is an efficient and user-friendly Python module that makes trying out multiple prediction algorithms a breeze. Its memory-efficient design and customizable options make it a valuable tool for any data scientist or machine learning enthusiast. I hope you enjoy using \u0026lsquo;EazyPredict\u0026rsquo; as much as I enjoyed creating it.\nCheck out the entire project on Github or PyPI .\n","date":"2 February 2023","permalink":"/projects/2023-02-03-eazypredict-module/","section":"Projects","summary":"EazyPredict - Running and comparing multiple ML models at once # Welcome to the world of \u0026lsquo;EazyPredict\u0026rsquo;, a Python module that aims to make trying out multiple prediction algorithms as simple and efficient as possible.","title":"EazyPredict ML module"},{"content":"","date":"2 February 2023","permalink":"/tags/from-scratch/","section":"Tags","summary":"","title":"from-scratch"},{"content":"","date":"2 February 2023","permalink":"/tags/k-nearest-neighbours/","section":"Tags","summary":"","title":"k-nearest-neighbours"},{"content":"","date":"2 February 2023","permalink":"/tags/machine-learning/","section":"Tags","summary":"","title":"machine-learning"},{"content":"","date":"2 February 2023","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":"2 February 2023","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"","date":"21 July 2022","permalink":"/tags/deep-learning/","section":"Tags","summary":"","title":"deep-learning"},{"content":"The aim of this blog is to guide individuals in comprehending the methodology and tactics required to construct a web scraper.\nThe problem with low resource languages for NLP # When getting started with an NLP project, the initial bottleneck is always acquiring relevant data. This issue is magnified when trying to work on an NLP project for Indian languages. Some of the challenges associated with building an NLP project for languages with a low digital presence are as follows.\nAbsence of Data: The absence of adequate data is the first problem. Digital text and audio data are frequently absent from low resource languages, making it difficult to train NLP algorithms. This reduces the amount of data that may be used for NLP system training, testing, and assessment. Lack of Pre-Trained Models: The absence of pre-trained models makes it more difficult to begin developing new models. Pre-trained models, which are often developed using enormous amounts of data, serve as a foundation for developing new models. Yet, such models might not exist or be too small for low resource languages to make a major impact. Language Complexity: Languages with limited resources are frequently more complex than their counterparts with abundant resources. They could lack standards in terms of spelling, grammar, or vocabulary, or they might have complex morphology or syntax. The restricted availability of linguistic resources, including dictionaries, grammars, and instruments for part-of-speech tagging and parsing, among other things, is another major issue. These tools are essential for developing NLP systems. Code-Mixing and Multilingualism: Low resource languages are often spoken in multilingual environments, leading to code-mixing or borrowing from other languages. This makes it challenging to develop models that can accurately capture the unique features of the language. Lack of Expertise: It takes specialist knowledge and experience to develop NLP solutions for languages with limited resources. This includes expertise in data science, computer learning, and languages. The availability of such knowledge, however, may be constrained, particularly in areas where the language is not frequently spoken or recognised. While this is pretty depressing, we can start fixing it one line of code at a time :).\nChoosing data sources # I’ve decided to focus on getting more data for Indian languages. More specifically I am focusing on Malayalam, which is my native language. The largest monolingual dataset that I have found for malayalam is the common crawl dataset . This might not be suitable for smaller projects as it is a collection of monthly scrapes of all internet data, in multiple languages.\nSo I decided to search for a smaller target, which led me to this website . This was a news website with a seperate section for archived data.This was a dream come true for a data nerd like me. All of this good, juicy data was just sitting there for the taking.\nThey had news articles saved all the way back to the year 2000!\nLets get scraping # The first step in scraping any website is to analyze the structure of the website and what kind of data you would like to extract from it. We will be using beautiful soup, and use it to parse html files.\nInstall bs4 using pip\npip install beautifulsoup4 For this particular website, there was archived news data for 8 different languages. Each of their websites had urls similar to “https://\u0026lt;language\u0026gt;.oneindia.com/archives/” where \u0026ldquo;\u0026lt;language\u0026gt;\u0026rdquo; was replaced appropriately with one of the 8 languages.\nThe archives itself was built like a calendar widget, where you could select a date and view the news for that particular date.\nNotice the changes in the url when we get to the day view. The url “https://malayalam.oneindia.com/2004/01/01/” displays the archived malayalam news data for January 1st 2007. This honestly makes our job so much easier as all we really have to do is loop through all the date combinations from 2000/05/01 to the current date. So our scraping function begins to look like this.\ndef get_data(url, lang, save_dir): url = url.replace(\u0026#34;language\u0026#34;, lang) end_date = date.today() start_date = date(2000, 5, 1) data = [] for single_date in daterange(start_date, end_date): url_end = single_date.strftime(\u0026#34;%Y-%m-%d\u0026#34;).replace(\u0026#34;-\u0026#34;, \u0026#34;/\u0026#34;) + \u0026#34;/\u0026#34; page_url = url + url_end # scrape data from page_url and return it get_data(url, args.lang, args.save_dir) The next step is to find the url information of each news article in the list. We can use this to navigate to each article and then finally get our data.\nThe built-in tools of our browser can help us in achieving this goal. Open developer tools to get a view of the html code displayed by the browser. Then using the inspect element tool, click on the news article. This highlights the code snippet relevant to the url.\npage_html = requests.get(page_url).content main_soup = BeautifulSoup(page_html, features=\u0026#34;html5lib\u0026#34;) for elem in main_soup.findAll(\u0026#34;ul\u0026#34;, attrs={\u0026#34;class\u0026#34;: \u0026#34;dayindexTitles\u0026#34;}): news_url = url + elem.a.attrs[\u0026#34;href\u0026#34;] Our initial step involves invoking an HTML request to the previously acquired URL. The resultant HTML content is subsequently passed to the BeautifulSoup wrapper, which utilizes the \u0026lsquo;html5lib\u0026rsquo; parser to construct a representation of the Document Object Model (DOM) of the page.\nThen we use the findAll function of beautifulSoup to find all ul elements with the class value of “dayIndexTitles”. The url can be accessed through the link element, with the href attribute.\nFinally to extract the main data, analyze the final page again.\nThe main data is in the \u0026lt;p\u0026gt; tags of the article. So we can utilize the following code to extract the necessary information and save them to a local file.\nnews_html = requests.get(news_url).content news_soup = BeautifulSoup(news_html, features=\u0026#34;html5lib\u0026#34;) for elem in news_soup.findAll(\u0026#34;p\u0026#34;): doc_data += str(elem).replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) data.append(doc_data) Finally put together the whole thing together and create a function as follows.\ndef get_data(url, lang, save_dir): url = url.replace(\u0026#34;language\u0026#34;, lang) end_date = date.today() start_date = date(2000, 5, 1) data = [] for single_date in daterange(start_date, end_date): url_end = single_date.strftime(\u0026#34;%Y-%m-%d\u0026#34;).replace(\u0026#34;-\u0026#34;, \u0026#34;/\u0026#34;) + \u0026#34;/\u0026#34; page_url = url + url_end for elem in main_soup.findAll(\u0026#34;ul\u0026#34;, attrs={\u0026#34;class\u0026#34;: \u0026#34;dayindexTitles\u0026#34;}): if elem.a: doc_data = \u0026#34;\u0026#34; news_url = url + elem.a.attrs[\u0026#34;href\u0026#34;] news_soup = BeautifulSoup( requests.get(news_url, headers=header).content, features=\u0026#34;html5lib\u0026#34; ) for elem in news_soup.findAll(\u0026#34;p\u0026#34;): doc_data += str(elem).replace(\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;) data.append(doc_data) Conclusion # Therefore our code is capable of extracting data from this website. The entire project code is available on GitHub . In the entire project, I have added some sanity checks and extended the functionality to a CLI, but the base logic of the scraping part remains the same.\n","date":"21 July 2022","permalink":"/projects/2022-07-21-malayalam-scraper/","section":"Projects","summary":"The aim of this blog is to guide individuals in comprehending the methodology and tactics required to construct a web scraper.\nThe problem with low resource languages for NLP # When getting started with an NLP project, the initial bottleneck is always acquiring relevant data.","title":"Developing a scraper to get NLP data using Beautiful Soup and python!"},{"content":"","date":"21 July 2022","permalink":"/tags/malayalam/","section":"Tags","summary":"","title":"malayalam"},{"content":"","date":"21 July 2022","permalink":"/tags/natural-language-processing/","section":"Tags","summary":"","title":"natural-language-processing"},{"content":" In this blog we go through the kNN algorithm logic, implement it from scratch in python and apply it on the MNIST digit dataset\nkNN algorithm explained intuitively! # Prerequisites # machine learning, supervised vs unsupervised ML, classification vs regression\nIntroduction # The K Nearest Neighbour classification is a simple and efficient machine learning model that can used for classification. While there are more complex classification models, if establishing a model with some training dataset is proving troublesome, it is likely that a kNN algorithm will provide the best solution. kNN is considered to be one of the top 10 data mining algorithms for this reason.\nkNN is not a model based classifier, instead it is called a lazy learner, as it performs classification tasks without building its model. In other words, the model does not have a “training” phase like other common machine learning models. The reasons for this will become apparent after I explain the model with an example.\nThis algorithm works on the concept of “similarity” between data points. This can easily be visualized by using a 2 dimensional dataset.\nExplanation with dummy data # Let’s say we wish to create a machine learning model that knows how to classify images of cats and dogs. For any ML model, we must have some data that highlights the different characteristics of both animals. Let us create this dataset ourselves, that would be pretty fun! The two characteristics that we choose could be as follows -\nThe sharpness of its claws The length of its ears Using these two features, we can end up with a model that has a reasonable accuracy. First, lets build a dataset for the problem. I’ll be doing this by measuring 4 legged mammals in my area ;). brb!\nDisclaimer - No animals were harmed in the creation of this blog\nLet\u0026rsquo;s say the data I have is as follows -\nAnimal Sharpness of its claws (cm) Length of its ears (cm) Cat 4 1 Cat 5 2 Cat 4 3 Dog 1 3 Dog 2 4 Dog 2 6 This data is classified into two classes, cat and dog. The data consists of two columns, the first one is called the label column. This identifies which class a data point is supposed to be in, and the second column is our features column.\nPlotting them results in a graph as shown below. Notice how similar animals are grouped “together” in the plot? This property is what kNN uses to perform classification.\nNow that we have our training data, let’s introduce a new unknown animal with the features (2.5, 4). Our model needs to able to determine if the unknown animal is a cat or a dog i.e. which class it belongs to. (Spoiler alert: it’s a cat! But our model doesn’t know that :p)\nThe nearest neighbour algorithm finds the data point that is “closest” to our unknown point. The way we measure this “closeness” can be through common methods, like euclidean or cosine distance. Here, I will be using the euclidean distance equation for the demonstration.\n$$ d(p,q) = sqrt{\\sum_{i=1}^n (q_i - p_i)^2} $$\nIf we calculate the euclidean distance from all other points and find the shortest distance, we can infer that the closest point to our unknown point is the point (2,4). This point refers to a cat, and so the nearest neighbour algorithm concludes that the unknown point is a cat. However, this method is extremely prone to overfitting, or in other words, if there are a few outliers in our data, the model’s prediction will suffer.\nWe overcome this issue by using the K Nearest Neighbour algorithm, which also happens to be the topic of this article :). What changes here is, instead of finding ONE shortest point, we find K points which are sorted in the order of their distance to our unknown point, in ascending order. From these K points, we look at which class value occurs the most, and then that will be the predicted class of the unknown point.\nLets take K=3 for our example. This will include the points given below. Since there are 2 points from the class “Cat”, and one point from the class “Dog”, our kNN model will predict (correctly) that our unknown animal is a cat.\nThat is the working of the kNN model on a very simple dataset. Now in real world machine learning problems you do not work on such simple datasets, but this example helps build our intuition of what the model does.1\nCoding kNN model from scratch # Now let’s create this model from scratch in python. Since it is a lazy learning algortihm, we do not need a training phase. We just need the training data and an un labelled instance for prediction. So the algorithm needs features, labels, and the data instance to be predicted as an input. We\u0026rsquo;ll choose an arbitrary value for K for this example.2 We also need a helper function to calculate the euclidean distance between two vectors.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # returns the euclidean distance between two vectors def euclidean_distance(vec1, vec2): distance=0 for i in range(len(vec1)): distance+=(vec1[i]-vec2[i])**2 return math.sqrt(distance) def knn_function(X_train, y_train, test_vector, k): distance=[] # loops through the training dataset, stores the training vector and corresponding label in a variable for idx in range(len(X_train)): img=X_train[idx] label=y_train[idx] # appends a tuple containing euclidean distance and label of the corresponding training vector distance.append((euclidean_distance(img, test_vector), label)) # sorts the tuples in the list in descending order, according to the euclidean distance sorted_dist=sorted(distance, key=lambda dist:dist[0]) # takes the first k values in the list i.e the k nearest neighbours neighbours=np.array(sorted_dist[:k]) return neighbours That\u0026rsquo;s the whole code! It really is a pretty simple learning algorithm. Now all we need a wrapper function to accept the input and return the prediction to the user.\n1 2 3 4 5 # this returns the predicted label i.e. the label that occurs the most def predict_class(test_vector, k=5): neighbours=knn_function(X_train, y_train, test_vector, k) labels, counts = np.unique(neighbours[:,1], return_counts=True) return labels[counts.argmax()] Please not that our implementation is not very optimized and really should be only used to learn how the model works. 3\nApplying the model on a standard dataset # As written on kaggle -\n\u0026ldquo;MNIST (\u0026ldquo;Modified National Institute of Standards and Technology\u0026rdquo;) is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\u0026rdquo;\nSo it\u0026rsquo;s a pretty standard dataset! Let\u0026rsquo;s use this to evaluate our model\u0026rsquo;s performance! First let\u0026rsquo;s get the dataset from kaggle and read it using pandas!\nThe shape of this data is (60000, 785). This means that each row is a vector containing a digit representation. That comes out to a total of 60000 images in the training dataset. The digit is represented as a 1x784 vector. So then why is the shape 785? This is because the last column is a label that stores what digit is represented by that particular row.\nThen we use numpy to store the data into train, test datasets.\n1 2 3 4 5 6 7 8 9 10 #split into features and labels X_train = np.array(train_data[train_data.columns.drop(\u0026#39;label\u0026#39;)]) y_train = np.array(train_data[\u0026#39;label\u0026#39;]) X_test = np.array(test_data[test_data.columns.drop(\u0026#39;label\u0026#39;)]) y_test = np.array(test_data[\u0026#39;label\u0026#39;]) # shuffle the data randomly rand_idx = np.random.permutation(X_train.shape[0]) X_train = X_train[rand_idx] y_train = y_train[rand_idx] Now let\u0026rsquo;s see what an image from this dataset looks like, and print the label.\n1 2 3 4 5 6 7 8 img=X_test[31] img = img.reshape(28, 28) plt.imshow(img, cmap=matplotlib.cm.binary) plt.axis(\u0026#39;off\u0026#39;) plt.show() print(y_test[31]) img.shape OUTPUT: 1.0 That looks pretty good! This means that the 31st vector stored in out test dataset corresponds to this image of the digit \u0026lsquo;1\u0026rsquo;. So now we have confirmed that the data is indeed stored and labelled properly. It\u0026rsquo;s time to unleash our kNN model on this dataset! Let us try to predict the result we get for the same test input. If everything goes well, we should get \u0026lsquo;1\u0026rsquo; as the predicted output.\nAnd we do! Our predict function predicted correctly that the image shown was that of a 1. Here also, the k value that we passed to the function was completely arbitrary. Now let\u0026rsquo;s properly evaluate the model on the entire test set.\nThe accuracy comes out to about 90%. Now I should say that this accuracy can be improved by selecting a better value of k, but it\u0026rsquo;s not bad for a model without any hyperparameter tuning.\nFootnotes # You could extend the same principles to higher dimensions. Here we have 2 features, so we only have to visualize 2 dimensions. But for most problems, it\u0026rsquo;s common for there to be a lot of dimensions, millions even. It’s difficult to imagine problems in a higher dimension, so working on them in lower dimensions while learning is a good idea! Quick detour on this topic here .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIt should be noted that there does exist algorithms for choosing an ideal value of K based on the training dataset. You can learn more here .\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOptimizations can be made to the algorithm such that some of the computation can be skipped, but this discussion is out of the scope of this article. Feel free to read up on it here \u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"30 January 2022","permalink":"/projects/2022-01-30-knn-algorithm-demystified/","section":"Projects","summary":"In this blog we go through the kNN algorithm logic, implement it from scratch in python and apply it on the MNIST digit dataset\nkNN algorithm explained intuitively! # Prerequisites # machine learning, supervised vs unsupervised ML, classification vs regression","title":"kNN algorithm explained intuitively!"},{"content":"Chrome Extension To Monitor Mental Health # Introduction # This project was a really interesting one that I was fortunate enough to be a part of! We made as an entry project to compete in a hackathon organised by HackOn. We did not win the hackathon, but we did create a unique and interesting project. (imo)\nOur project aims to monitor a user\u0026rsquo;s search patterns and use this information to inform their close/loved ones about potential mental health problems the user might be facing. A recent study showed that most people who are mentally depressed in some form have used the internet in a manner that has worsened their illness. So our goal with this approach is to help in the monitoring of ‘at-risk’ individuals and prevent them from becoming their own worst enemy.\nWe aim to achieve this goal through a chrome extension that tracks the search phrases entered by the user and sends them to a deep learning model, that determines whether the user shows signs of depression or not!\nThought Process Behind the Project # The motivation behind this project is highly personal. We all have a friend who had to go through some dark times in some periods of their lives. Some were able to overcome this, while some were not so fortunate. One thing that we all can agree on is that there are not many support systems that help or support people in such cases. Our extension is aimed to help not only the fighters but also their supporters who want to help more. We know someone who went through such a phase, and something that was missing was a medium to alert close ones when to intervene, as you can never expect someone to know everything you are going through.\nMaking the machine learning model # I was in charge of making the machine learning model that would be used by us for the web extension. I used common NLP methods such as preprocessing, tokenizing, encoding. The pre-processing was done by making a function of its own.\nDataset # We used two different datasets for this model. The problem is a classification question that has to decide whether a person is depressed (true state) or not depressed (false state). I used 2 datasets to train the true and false state.\nThe first dataset is from kaggle. It is a collection of posts from \u0026ldquo;SuicideWatch\u0026rdquo; and \u0026ldquo;depression\u0026rdquo; subreddits of the Reddit platform. The posts are collected using Pushshift API. All posts that were made to \u0026ldquo;SuicideWatch\u0026rdquo; from Dec 16, 2008 (creation) till Jan 2, 2021, were collected while \u0026ldquo;depression\u0026rdquo; posts were collected from Jan 1, 2009, to Jan 2, 2021. We took the posts that were written by people suffering from depression as the true state.\nThe second dataset is a sentiment analysis dataset. It contains 1,600,000 tweets extracted using the twitter API. The tweets have been annotated (0 = negative, 2 = neutral, 4 = positive) and they can be used to detect sentiment. We took the dataset that includes neutral and positive flags.\nI did it this way because the \u0026rsquo;negative\u0026rsquo; statements from the twitter sentiment analysis dataset might be some tweet regarding a hate crime, or an angry tweet. It does not necessarily have to be the words of a person undergoing depression. So I overcame this with the Reddit dataset. Going through the subreddit mentioned, it was clear that most of the posts were regarding depression/existential crisis. Any post swaying from these topics would be quickly taken down by the moderators. So, this was most accurate data we would get that could be used to emulate the psyche of a depressed person.\nPre-proccessing- # TEXT_CLEANING_RE = \u0026#34;@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\u0026#34; def preprocess(text, stem=False): # Remove link,user and special characters text = re.sub(TEXT_CLEANING_RE, \u0026#39; \u0026#39;, str(text).lower()).strip() tokens = [] for token in text.split(): if token not in stop_words: if stem: tokens.append(stemmer.stem(token)) else: tokens.append(token) return \u0026#34; \u0026#34;.join(tokens) I used packages from word2vec and keras text pre-processing for building the vocabulary and tokenizing the data respectively.\nBuilding vocabulary - # W2V_SIZE = 300 W2V_WINDOW = 7 W2V_EPOCH = 32 W2V_MIN_COUNT = 10 KERAS_MODEL = \u0026#34;model.h5\u0026#34; WORD2VEC_MODEL = \u0026#34;model.w2v\u0026#34; TOKENIZER_MODEL = \u0026#34;tokenizer.pkl\u0026#34; ENCODER_MODEL = \u0026#34;encoder.pkl\u0026#34; w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8) w2v_model.build_vocab(documents) w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH) Tokenizing - # tokenizer = Tokenizer() tokenizer.fit_on_texts(df_train.text) vocab_size = len(tokenizer.word_index) + 1 print(\u0026#34;Total words\u0026#34;, vocab_size) Then I used LabelEncoder() from sklearn to encode the tokenized text data.\nEncoding - # encoder = LabelEncoder() encoder.fit(df_train.target.tolist()) y_train = encoder.transform(df_train.target.tolist()) y_test = encoder.transform(df_test.target.tolist()) y_train = y_train.reshape(-1,1) y_test = y_test.reshape(-1,1) Making the model # We defined the neural network as follow and trained it for about 10 epochs. This resulted in an accuracy of 96%.\nmodel = Sequential() model.add(embedding_layer) model.add(Dropout(0.5)) model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.summary() model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#34;adam\u0026#34;, metrics=[\u0026#39;accuracy\u0026#39;]) After the training is done, we can predict if a phrase shows signs of depression or not, in the following way -\npredict(\u0026#34;painless way to die\u0026#34;, model1) {\u0026#39;elapsed_time\u0026#39;: 0.1011207103729248, \u0026#39;label\u0026#39;: True, \u0026#39;score\u0026#39;: 0.8831092119216919} Making the chrome extension # I was not involved very heavily in this part, but what we did was to design a front end that takes in a users email ID, name and multiple friend\u0026rsquo;s email IDs. The extension is built using javascript and closely interacts with Chrome extensions API. This makes it possible to run on any chromium based browser such as opera, edge, etc. The extension gets the search query and sends it as a JSON structure to our API endpoint /predict. This endpoint is where our DL model is hosted. The DL model takes the string as an input and returns a score (from 0-1 ). The higher the score is, the more likely the person is to be depressed. This extension runs in the background and updates a score variable based on the averages of multiple search results. If the score variable crosses a particular threshold, an email will be sent to the close ones of the user.\nThat was the method we used to implement this project :). Link to the GitHub repo can be found here. ","date":"31 May 2021","permalink":"/projects/2021-05-31-notblue-chrome-extension/","section":"Projects","summary":"Chrome Extension To Monitor Mental Health # Introduction # This project was a really interesting one that I was fortunate enough to be a part of! We made as an entry project to compete in a hackathon organised by HackOn.","title":"Chrome Extension To Monitor Mental Health"},{"content":"","date":"31 May 2021","permalink":"/tags/machine-learning-natural-language-processing-deep-learning-chrome-extension/","section":"Tags","summary":"","title":"machine-learning, natural-language-processing, deep-learning, chrome-extension"},{"content":"Building a Football Score Updates API with Node.js and Web Scraping # Football Score Updates API # In the world of football, staying up to date with the latest match scores and updates is crucial for fans, sports journalists, and developers alike. In this technical blog post, we\u0026rsquo;ll explore how to build a Football Score Updates API using Node.js and web scraping techniques. Our API will fetch real-time match information from the popular Flash Score website, allowing users to access comprehensive data on matches happening in various countries and leagues.\nTechnologies Used # To create our Football Score Updates API, we\u0026rsquo;ll leverage the following technologies:\nNode.js: A powerful JavaScript runtime environment that allows us to build server-side applications.\nExpress: A popular Node.js framework for building web APIs that simplifies routing and request handling.\nPuppeteer: A Node library which provides a high-level API to control headless Chrome or Chromium over the DevTools Protocol.\nSetting Up the Project # Let\u0026rsquo;s start by setting up our project:\nInitialize a new Node.js project by running npm init in your project directory. Follow the prompts to create a package.json file.\nInstall the required dependencies by running the following command:\nnpm install express cors puppeteer Create an index.js file in your project directory to serve as the entry point for our application.\nOpen index.js and import the required modules:\nimport express from \u0026#39;express\u0026#39;; import cors from \u0026#39;cors\u0026#39;; Initialize an Express application and set the port:\nconst app = express(); const PORT = process.env.PORT || 3000; Enable CORS middleware to handle Cross-Origin Resource Sharing:\napp.use(cors()); Define a route in your Express application to handle the match updates request, and relegate the scraping work to another function to maintain code cleanliness.\napp.get(\u0026#39;/:country/:comp\u0026#39;, async (req, res, next) =\u0026gt; { try { const { country, comp } = req.params; const data = await getDataFromCompName(country, comp); res.status(200).json(data); } catch (error) { next(error); } }); Start the server by calling app.listen(PORT, \u0026hellip;):\napp.listen(PORT, () =\u0026gt; console.log(`🚀 Server ready at http://localhost:${PORT}/`)); Now, our Express server is ready to handle incoming requests and fetch match updates using the getDataFromCompName function.\nWeb Scraping the Flash Score Website # Next, we\u0026rsquo;ll implement the web scraping logic using Puppeteer to fetch the latest match updates from the Flash Score website.\nDefine a route in your Express application to handle the match updates request:\napp.get(\u0026#39;/matches\u0026#39;, async (req, res) =\u0026gt; { try { const { country, league } = req.query; const data = await getDataFromCompName(country, league); res.json({ matches: data }); } catch (error) { console.error(error); res.status(500).json({ error: \u0026#39;An error occurred\u0026#39; }); } }); Implement the getDataFromCompName function to perform the web scraping:\nasync function getDataFromCompName(country, league) { const url = `https://www.flashscore.in/football/${country}/${league}`; try { const browser = await puppeteer.launch(); const page = await browser.newPage(); await page.setRequestInterception(true); page.on(\u0026#39;request\u0026#39;, (req) =\u0026gt; { if (req.resourceType() === \u0026#39;font\u0026#39; || req.resourceType() === \u0026#39;image\u0026#39;) { req.abort(); } else { req.continue(); } }); await page.goto(url, { waitUntil: \u0026#39;networkidle0\u0026#39; }); const data = await page.evaluate(() =\u0026gt; { const sections = document.querySelector(\u0026#39;section.event.event--live.event--summary\u0026#39;); const divs = Array.from(sections.getElementsByTagName(\u0026#39;div\u0026#39;)); const match_info = []; for (let i = 0; i \u0026lt; divs.length; i += 1) { const current = divs[i]; if (current.id !== \u0026#39;\u0026#39;) { match_info.push(current.innerText.split(\u0026#39;\\n\u0026#39;)); } } return match_info; }); await browser.close(); return parseJSON(data); } catch (error) { console.error(error); return []; } } Implement the parseJSON function to process the scraped data:\nfunction parseJSON(match_info) { const final_json = []; if (match_info.length === 0) { return [\u0026#39;No matches have taken place in the last 24 hours.\u0026#39;]; } match_info.forEach((match) =\u0026gt; { const match_info_json = {}; final_json.push(new MatchInstance(match_info_json)); }); return final_json; } Define the MatchInstance class to represent a single match instance:\nclass MatchInstance { constructor(match_info_json) { this.status = match_info_json.status; this.time = match_info_json.time; this.team_1 = match_info_json.team_1; this.team_2 = match_info_json.team_2; this.score_1 = match_info_json.score_1; this.score_2 = match_info_json.score_2; this.fh_score = match_info_json.fh_score; } } Testing the API # Start the Express server by adding the following code to the end of index.js:\nconst port = process.env.PORT || 3000; app.listen(port, () =\u0026gt; { console.log(`Server listening on port ${port}`); }); Run the API by executing the following command in your terminal:\nnode index.js Open your terminal and provide a curl request to \u0026ldquo;http://localhost:3000/matches?country=your_country\u0026amp;league=your_league\u0026rdquo; to retrieve the latest match updates for your desired country and league.\nYou should receive a JSON response containing the match data.\nCustomization and Enhancements # Our basic Football Score Updates API is now up and running. However, there are several customization and enhancement options you can explore:\nAdditional Endpoints: Extend the API to provide additional endpoints for accessing team information, player statistics, or historical match data. Error Handling: Implement robust error handling to provide clear and meaningful error messages to API consumers. Caching: Introduce a caching mechanism to reduce the number of requests made to the Flash Score website and improve the API\u0026rsquo;s performance. Authentication: Implement authentication mechanisms, such as API keys or JWT tokens, to secure the API and control access to the match data. Conclusion # In this blog post, we\u0026rsquo;ve explored how to build a Football Score Updates API using Node.js and Puppeteer. By leveraging the power of Express and Puppeteer, we were able to scrape real-time match updates from the Flash Score website and deliver them as JSON responses. This API serves as a foundation for providing football enthusiasts with the latest match information, enabling them to stay connected to the game they love.\nFeel free to explore further and customize the API according to your specific requirements. Happy coding!\nNote: When scraping websites, it\u0026rsquo;s important to be mindful of their terms of service and to respect their usage policies. Always ensure that you\u0026rsquo;re scraping responsibly and within legal and ethical boundaries.\n","date":"21 July 2020","permalink":"/projects/2020-07-21-football-api/","section":"Projects","summary":"Building a Football Score Updates API with Node.js and Web Scraping # Football Score Updates API # In the world of football, staying up to date with the latest match scores and updates is crucial for fans, sports journalists, and developers alike.","title":"Building a Football Score Updates API with Node.js and Web Scraping"},{"content":"","date":"21 July 2020","permalink":"/tags/node-js-web-scraping-api-javascript/","section":"Tags","summary":"","title":"node-js, web-scraping, api, javascript"},{"content":"AMFOSS TASKS # \u0026ldquo;Success is normally found in a pile of mistakes.\u0026rdquo; - Tim Fargo\nIf there was ever a point in my life where I felt this quote, it is after I started doing the tasks required for joining the amFOSS club at Amrita Vishwa Vidyapeetham.\nAmFOSS is an open source club consisting of a few motivated students promoting and contributing to free and open source software. This helps students to learn out of academics and get introduced to the outside world. So, quite naturally I was interested in joining the club. I came to know about this club through quora. There were a lot of posts on the club(which I found out later was a result of foss members spamming the website).\ntask-1,2 # My first direct introduction to amFOSS, however was through the induction ceremony organized by the college. I registered for the club immediately, and soon got the tasks. At first, I was feeling quite good about the tasks. Task 1 was a breeze, all I had to do was run an automated script on GitHub. Task 2 was a bunch of typical programming challenges. I had learnt python during my vacations as I had way too much free time on my hand. That came in handy and I did the challenges using python. TASK 1 TASK 2 task 3 # After these, I started working on task 3. This task was very close to making me successfully pull my hair out. I had to create a program for scraping google search using ruby and nokogiri. I read through most of the documentation of ruby and nokogiri. I also scraped a smaller website as a part of a blog post tutorial. So I was feeling quite confident in cracking this task; which was until I actually started doing the task. I had underestimated the complexity of a google search page, and no matter what I tried, I could not parse any sort of useful result from it. I immediately pinned this on my lack of knowledge of HTML and CSS. To rectify this issue, I started learning HTML and CSS from an online source. It took me about two days to learn the syntaxes of HTML and CSS.(Even though it was explicitly mentioned in the pdf given to us to not get sidetracked into learning a whole language, I couldn\u0026rsquo;t stop until I felt that I had knowledge of the basics). This did give me clearer understanding of the problem and made the use of nokogiri much easier, but at the end of the day, I had no results to display. It almost seemed to me that the google search page was built in a way to prevent scraping. That is as far a progress I got in that task. TASK 3 task-4 # Dejected at not being able to solve the task, I moved on to the next task, \u0026ldquo;Advanced XOR\u0026rdquo;. I was completely new to encryption of all sorts. So, I read about encryption and learnt what the terms key, check hash, ciphertext stood for. Then I proceeded to read the encryption script. I tried doing it for a bit of time, but then I simply could not understand it.\ntask-5 # This brings us to the next task. I started this task with apprehension because graphQL was a relatively new language. I started out with reading about API\u0026rsquo;s and the different ways they are used to query information. Then I read about rEST API, I understood that graphQL was an improvement to rEST since several instances of data could be returned in a single query compared to rEST which requires multiple queries to get the same results. After aquiring this knowledge I started working on my website. This proved to be relatively simple and I finished it in about a day. I also read about graphQL on multiple websites and since there wasn\u0026rsquo;t much information on it, I relied heavily on it\u0026rsquo;s official documentation. Graphiql explorer was a fun way of trying graphQl and I started experimentation. Since the syntax was easy, I got the query working there. This is where I ran into my blockers. I had two main blockers in this task and they haunt me to this day. They were - i) Authenticating the query from javascript. ii) Implementing graphQL in javascript. This took up about 1/3rd of my total time and was a huge pain in the ass. I looked at a variety of libraries and clients to resolve the issue. They include graphql.js, nodejs, apollo client for graphql. I even went as far trying to execute it through a python script using django after reading through the method in which they implemented it on GitLit repositary on amFOSS directly. Needless to say, I learnt how not to approach an issue through this task. I wasted a lot of time on this that could be used in other tasks. TASK 5 task 6,7 # I made negligible progress in tasks 6,7. All I did was study the syntax of rust and installed it on my laptop.\ntask 8 # Captcha breaking was a very simple task and it was a welcome addition after task 5. All I did was install a couple of packages from google.(Tesseract OCR). After that it was fairly straightforward to get the text from images using the OCR. TASK 8 task 9 # Creating a website using jekyll themes was also pretty straightforward. Here, the knowledge I got by learning HTML, CSS came into handy and editing the website was a breeze. I found out this super cool minimalistic theme from jekyll themes, forked it and got a website without much fuss. TASK 9 ","date":"20 August 2019","permalink":"/posts/2019-08-20-amfoss-tasks/","section":"Posts","summary":"AMFOSS TASKS # \u0026ldquo;Success is normally found in a pile of mistakes.\u0026rdquo; - Tim Fargo\nIf there was ever a point in my life where I felt this quote, it is after I started doing the tasks required for joining the amFOSS club at Amrita Vishwa Vidyapeetham.","title":"Applying to AMFOSS open source club"},{"content":"","date":"20 August 2019","permalink":"/tags/beginnner/","section":"Tags","summary":"","title":"beginnner"},{"content":"","date":"20 August 2019","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"20 August 2019","permalink":"/tags/programming/","section":"Tags","summary":"","title":"programming"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]